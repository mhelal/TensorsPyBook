{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e49fd61",
   "metadata": {},
   "source": [
    "# Ch3: Geometry & Algebra of Tensors\n",
    "    \n",
    "## 3.1 Motivation and Intuition\n",
    "\n",
    "A Video Analysis with Tensor Decomposition in Python example can be found at:\n",
    "https://towardsdatascience.com/video-analysis-with-tensor-decomposition-in-python-3a1fe088831c\n",
    "\n",
    "Other Examples are presented below, using a tensorisation step that is hand tailored to each examples, with attempts to generalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdea736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def tensorisation_Values (Df, components, value=\"value\", aggFunc=np.mean):\n",
    "    if components >= Df.shape[1]:\n",
    "        print (\"Number of components must be less or equal to number of the columns in the input matrix. Exiting without creating the tensor\")\n",
    "        return\n",
    "    minVals = []\n",
    "    maxVals = []\n",
    "    tensorShape = []\n",
    "    for i in range(components):\n",
    "        minVal = Df.iloc[:,i].min()\n",
    "        # this will be an index, therefore starting from zero is necessary\n",
    "        if minVal > 0:\n",
    "            Df.iloc[:,i] += minVal\n",
    "            minVal = 0\n",
    "        if minVal < 0:\n",
    "            Df.iloc[:,i] -= minVal\n",
    "            minVal = 0\n",
    "        minVals.append(minVal)\n",
    "        # also the max value need to be positive non-zero, because it will be the tensor shape \n",
    "        maxVal = Df.iloc[:,i].max()\n",
    "        if maxVal <= 0:\n",
    "            Df.iloc[:,i] -= maxVal + 1\n",
    "            maxVal = 1\n",
    "        maxVals.append(maxVal)\n",
    "        print(\"mode  \" + str(i) + \" max value =\" + str(maxVal) + \", min value = \" + str(minVal))\n",
    "        tensorShape.append(int(maxVal)+1)\n",
    "\n",
    "    # update the values in the array, to be used as indices\n",
    "    for k,j in Df.iterrows():\n",
    "            for i in range(components):\n",
    "                j[i] = int(math.floor(j[i])) + abs(int(minVals[i])) + 1\n",
    "    print (tensorShape)\n",
    "    tensorShape = tuple(tensorShape) \n",
    "    tensor_array = np.zeros(tensorShape)\n",
    "    count = 0\n",
    "    for k,j in Df.iterrows():\n",
    "        count = count + 1\n",
    "        t_index = tuple(\n",
    "            int(j[i]) for i in range(components)\n",
    "        )\n",
    "        #print (t_index)\n",
    "        tensor_array[t_index] = aggFunc(j[value])\n",
    "\n",
    "    return tensor_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cbd81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "\n",
    "userdata = pd.read_csv('data/GlobalLandTemperaturesByMajorCity.csv')\n",
    "columns = ['dt','AverageTemperature','AverageTemperatureUncertainty','City','Country','Latitude','Longitude'] # such that these will be x\n",
    "df=userdata[columns].copy()\n",
    "Values = userdata[\"AverageTemperature\"] # this will be the values of the tensor\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ead59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will tensorise in the same order mentioned in chapter 3 first section on motivational problem\n",
    "# First example is one temperature value as a scalar (rank-zero tensor), which does not require tensorisation, just indexing will do\n",
    "df.iloc[0, 1] # specifying the index values, which is not usually interpreted easily\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3d0571",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['City']== 'Abidjan') & (df['dt']=='1849-01-01') ,['AverageTemperature']] # specifing the values needed (city and date values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67223653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second example is all temperature values in this dataset or in a specific city as a vector (rank-one tensor), which does not require tensorisation, just indexing will do\n",
    "\n",
    "df.loc[df['City'] == 'Abidjan']['AverageTemperature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9d8d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third example is temperature values per city (rank-two tensor). We will do it using pivot tables, and again with tensorisation\n",
    "# pivot tables are another method in handling multi-way analysis\n",
    "# checking how many cardiac cases in each age value\n",
    "\n",
    "cityTempMeans = df.pivot_table(values='AverageTemperature',\n",
    "                               index='City', \n",
    "                               aggfunc=np.mean,               ## aggfunc='size', # size if you want to aggregate by frequency counting\n",
    "                               fill_value=0)\n",
    "cityTempMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ccd803",
   "metadata": {},
   "outputs": [],
   "source": [
    "cityTempMeans.iloc[0,0] # Average temperature for Abidjan by indexing the pivot table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a94c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['City'] == 'Abidjan']['AverageTemperature'].mean() # getting to the same value from the original dataset DataFrame, probably the mean function used here is not np.mean since the value is slightly different, but could be rounding error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac54ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    print(sdf.iloc[:,i].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e55177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorising by arranging the columns as coordinates first, and last column is the value to aggregate on\n",
    "\n",
    "columns = ['City','AverageTemperature'] # such that these will be x\n",
    "sdf=userdata[columns].copy()\n",
    "\n",
    "# we need to encode all coordinate columns numerically\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# lbl_encoder object knows how to understand word labels.\n",
    "city_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "sdf['CityBasis']= city_encoder.fit_transform(sdf['City'])\n",
    "sdf['CityBasis'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f34614",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca6fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_encoder.inverse_transform([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5774dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_encoder.transform(['Abidjan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462c78f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['CityBasis', 'City','AverageTemperature'] # such that these will be x\n",
    "sdf=sdf[columns].copy()\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c4ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly as tl\n",
    "\n",
    "tensor_array = tensorisation_Values (sdf, 1, value=\"AverageTemperature\", aggFunc=np.mean)\n",
    "tensor2 = tl.tensor(tensor_array)\n",
    "tensor2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafafc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor2 # the tensorisation function can also be updated to handle nan values, and to be vectorised and optimisaed for parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f309f80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourth example is temperature values per location as Latitude & Longitude  (rank-three tensor). \n",
    "#Attemping pivot tables, will still be rank-two, as we create a matrix with the both location columns flattened in one mode\n",
    "columns = ['dt','AverageTemperature','AverageTemperatureUncertainty','City','Country','Latitude','Longitude'] # such that these will be x\n",
    "\n",
    "\n",
    "locTempMeans = df.pivot_table(values='AverageTemperature',\n",
    "                               index=['Latitude','Longitude'], \n",
    "                               aggfunc=np.mean,               ## aggfunc='size', # size if you want to aggregate by frequency counting\n",
    "                               fill_value=0)\n",
    "locTempMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cdd63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will do it using tensorisation\n",
    "\n",
    "columns = ['Latitude','Longitude','AverageTemperature'] \n",
    "sdf=userdata[columns].copy()\n",
    "\n",
    "sdf # 'Latitude','Longitude' contain numeric values followed by N/S in the first, and E/W in the second, which is degree minute second (DMS) coordinates \n",
    "# We will need to be numerically encode them to be turned to coordinate basis using decimal degrees\n",
    "# there is a solution here https://medium.com/@quinn.dougherty92/simple-geographical-encoding-8293fde9e964"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9067179",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dms2dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b708241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geopandas has interesting solutions, but will use dms2dec for simplicity\n",
    "from dms2dec.dms_convert import dms2dec\n",
    "sdf['Latitude'] = sdf['Latitude'].apply(dms2dec)\n",
    "sdf['Longitude'] = sdf['Longitude'].apply(dms2dec)\n",
    "sdf['Latitude'] = sdf['Latitude'].astype(float)\n",
    "sdf['Longitude'] = sdf['Longitude'].astype(float)\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191567bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will need to update the tensorisation function to be able to aggregate by mean or other function, and take\n",
    "# the values as mean of rows returned from a condition per column in the dataframe\n",
    "# this is almost tailored to Latitude & Longitude specific values, it is better to be updated to ranges, such that\n",
    "# values are aggregated when they are >= the current basis index and < next basis index\n",
    "# many optimisations can be achived for vectorisation, paralleisation, quantisation\n",
    "def tensorisation2_Values (Df, components, value=\"value\", aggFunc=np.mean):\n",
    "    if components >= Df.shape[1]:\n",
    "        print (\"Number of components must be less or equal to number of the columns in the input matrix. Exiting without creating the tensor\")\n",
    "        return\n",
    "    minVals = []\n",
    "    maxVals = []\n",
    "    tensorShape = []\n",
    "    for i in range(components):\n",
    "        minVal = Df.iloc[:,i].min()\n",
    "        # this will be an index, therefore starting from zero is necessary\n",
    "        if minVal > 0:\n",
    "            Df.iloc[:,i] += minVal\n",
    "            minVal = 0\n",
    "        if minVal < 0:\n",
    "            Df.iloc[:,i] -= minVal\n",
    "            minVal = 0\n",
    "        minVals.append(minVal)\n",
    "        # also the max value need to be positive non-zero, because it will be the tensor shape \n",
    "        maxVal = Df.iloc[:,i].max()\n",
    "        if maxVal <= 0:\n",
    "            Df.iloc[:,i] -= maxVal + 1\n",
    "            maxVal = 1\n",
    "        maxVals.append(maxVal)\n",
    "        print(\"mode  \" + str(i) + \" max value =\" + str(maxVal) + \", min value = \" + str(minVal))\n",
    "        tensorShape.append(int(maxVal)+1)\n",
    "\n",
    "    # update the values in the array, to be used as indices\n",
    "    for k,j in Df.iterrows():\n",
    "            for i in range(components):\n",
    "                j[i] = int(math.floor(j[i])) + abs(int(minVals[i])) + 1\n",
    "    print (tensorShape)\n",
    "    tensorShape = tuple(tensorShape) \n",
    "    tensor_array = np.zeros(tensorShape)\n",
    "    \n",
    "    ## two useful functions\n",
    "    np.unravel_index(0, tensorShape) # flat linear index to multidimensional index \n",
    "    np.ravel_multi_index([tensorShape[i]-1 for i in range(len(tensorShape))], tensorShape) # multidimensional index to flat linear index\n",
    "\n",
    "    for i in range (np.prod(tensorShape)-1): # iterate through the tensor flat indices\n",
    "        t_index = np.unravel_index(i, tensorShape) # get the tensor multidimensional index\n",
    "        print (\"i: \" + str(i) + \" t_in: \" + str(t_index))\n",
    "        condition = '' # accumulate the conditions to add to the data frame selection\n",
    "        for j in range(len(t_index)):       # iterate through the dataframe columns\n",
    "            if j==0:\n",
    "                condition += '(Df[Df.columns[' + str(j)+']] == ' + str(t_index[j]) + ')'\n",
    "            else:\n",
    "                condition += ' & (Df[Df.columns[' + str(j)+']]  == ' + str(t_index[j]) + ')'\n",
    "            print (\"j: \" + str(j) + \" column: \" + Df.columns[j] + \" cond: \" + condition)\n",
    "            \n",
    "        #print (\"Condition is: \" + condition)\n",
    "        if len(condition) > 0:    \n",
    "            #print (eval(condition))\n",
    "            tensor_array[t_index] = aggFunc(Df.loc[eval(condition)]['AverageTemperature'])\n",
    "        else:\n",
    "            tensor_array[t_index] = aggFunc(Df.loc['AverageTemperature'])\n",
    "\n",
    "\n",
    "    return tensor_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca50ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_array =  tensorisation2_Values (sdf, 2, value=\"AverageTemperature\", aggFunc=np.mean)\n",
    "tensor2 = tl.tensor(tensor_array)\n",
    "tensor2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb5da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fifth example is temperature values per location as Latitude & Longitude & date  (rank-four tensor). We will do it using tensorisation\n",
    "#Attemping pivot tables, will still be rank-two, as we create a matrix with the both location columns and time column flattened in one mode\n",
    "columns = ['dt','AverageTemperature','AverageTemperatureUncertainty','City','Country','Latitude','Longitude'] # such that these will be x\n",
    "\n",
    "\n",
    "locTempMeans = df.pivot_table(values='AverageTemperature',\n",
    "                               index=['Latitude','Longitude','dt'], \n",
    "                               aggfunc=np.mean,               ## aggfunc='size', # size if you want to aggregate by frequency counting\n",
    "                               fill_value=0)\n",
    "locTempMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2484246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will do it using tensorisation\n",
    "\n",
    "columns = ['Latitude','Longitude','dt', 'AverageTemperature'] \n",
    "sdf=userdata[columns].copy()\n",
    "\n",
    "sdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have here a date column, we can create time series by lagging function and quantisation of values, but will simply encode here\n",
    "date = pd.to_datetime(sdf['dt'])\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c979178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf['Latitude'] = sdf['Latitude'].apply(dms2dec)\n",
    "sdf['Longitude'] = sdf['Longitude'].apply(dms2dec)\n",
    "sdf['Latitude'] = sdf['Latitude'].astype(float)\n",
    "sdf['Longitude'] = sdf['Longitude'].astype(float)\n",
    "\n",
    "date_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "sdf['dtBasis']= date_encoder.fit_transform(pd.to_datetime(sdf['dt']))\n",
    "sdf['dtBasis'] = sdf['dtBasis'].astype(float)\n",
    "sdf['dtBasis'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35495cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Latitude','Longitude','dtBasis', 'AverageTemperature'] \n",
    "sdf=sdf[columns].copy()\n",
    "\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc74e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_array =  tensorisation2_Values (sdf, 3, value=\"AverageTemperature\", aggFunc=np.mean)\n",
    "tensor2 = tl.tensor(tensor_array)\n",
    "tensor2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d597253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13491ebe",
   "metadata": {},
   "source": [
    "## We will consider another problem using Data already in tensor form.\n",
    "\n",
    "\n",
    "The data set is from http://www.models.life.ku.dk/nwaydata\n",
    "in Matlab form, and can be read by scipy loadmat function, and saved as numpy arrays for easier loads later\n",
    "\n",
    "The data has X variable as 3-way tensor of  5 samples in mode - 1 (rows) 5 × 51 × 201. , containing different amounts of tyrosine, tryptophan and phenylalanine amino acids belong to three amino acids dissolved in phosphate buffered water. The samples were measured by fluorescence (excitation 250-300 nm, emission 250-450 nm, 1 nm intervals) on a spectrofluorometer \n",
    "\n",
    "The data has Y variable, which is the ground truth,  the known concentrations of the three chemicals (mode-2) that are in the samples (mode-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea70ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "amino = scipy.io.loadmat('data/amino.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b1c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = amino.get('X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5922a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = amino.get('Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8efe0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save('data/amino_x', X)\n",
    "np.save('data/amino_y', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f58eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab25753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a1337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing Parafac decomposition\n",
    "from tensorly.decomposition import parafac\n",
    "weights, factors = parafac(X,rank = 3, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466780dc",
   "metadata": {},
   "source": [
    "## PARAFAC should have three components and therefore a 5 × 3 so called score matrix (first mode loading matrix). Each column in this score matrix should approximately match the concentration of one of the three aminoacids which are held in the 5 × 3 Y matrix. Matching in this case, means that the corresponding columns should be correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9189c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7feff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[f.shape for f in factors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ceb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isclose(Y, factors[0]) # obviously they are not close enough for numpy, will do all possible column permutations correlation then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33be6ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def corrEstimate (i, j, Y, factors):\n",
    "    data = {\n",
    "        'Y': Y[:,i], \n",
    "        'Y_p': factors[0][:,j]\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Y', 'Y_p'])\n",
    "    corr = df.corr()\n",
    "    print(\"Correlation matrix of estimated column\" + str(i) + \" with ground truth \" + str(j) + \" is : \")\n",
    "    print(corr)\n",
    "    \n",
    "    return corr, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e876bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "ax = 0\n",
    "corrList = []\n",
    "for i in range(Y.shape[1]):\n",
    "    for j in range(factors[0].shape[1]):\n",
    "        corr, ax = corrEstimate(i, j, Y, factors)\n",
    "        corrList.append(corr)\n",
    "\n",
    "n=0 # there is a bug that makes the plot read the same correlation matrix every time, I tried inside the function, and then added the list to separate the variables, and not good\n",
    "for i in range(Y.shape[1]):\n",
    "    for j in range(factors[0].shape[1]):\n",
    "        ax = plt.subplot(3, 3, n+1)\n",
    "        plt.imshow(corrList[n],cmap='coolwarm',interpolation='nearest')\n",
    "        n= n+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef678b3",
   "metadata": {},
   "source": [
    "### It is obvious that the first estimated chemical matches the first ground truth, more than the other two\n",
    "\n",
    "### The second estimated chemical matches the third ground truth, \n",
    "\n",
    "### The third estimated checmical matches the second ground truth\n",
    "\n",
    "### The second and third are swapped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf1cd04",
   "metadata": {},
   "source": [
    "# Ch6: Fundamentals of Tensor Analysis & Applications\n",
    "\n",
    "## 6.1 Scientific Computing Applications\n",
    "\n",
    "## Second Problem is a survey answers about happiness scores related to 3 conditions. This created a three-way contingency table has as its variables: Happiness reported by participants in a survey (i mode-1 : 3 categories), Number of siblings (j mode-2 : Siblings - 5 categories) and the Years of schooling completed (k mode-3 : Schooling - 4 categories). It is thus a frequency table containing in the cells the number of people with a particular combination of categories. \n",
    "\n",
    "## the data is stored in a dat file that reads a 2 dimensional matricised tensor, (12, 5), assuming that reshaping into (3, 4, 5) and swapping the mode-2 and mode-3 in the problem definition in https://three-mode.leidenuniv.nl/ will be ok\n",
    "\n",
    "## Will decompose with Tucker to find which rank gives the best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f2e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# read flash.dat to a list of lists\n",
    "datContent = [i.strip().split() for i in open(\"data/Happiness.dat\").readlines()]\n",
    "\n",
    "X = np.array(datContent)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d7949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(3, 4, 5)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98b4df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing Tucker decomposition\n",
    "from tensorly.decomposition import tucker\n",
    "import tensorly as tl\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "def doTucker (X, rank):\n",
    "    X = tl.tensor(X,dtype=float)\n",
    "    core, factors = tucker(X, rank=rank)\n",
    "    tucker_reconstruction_2 = tl.tucker_to_tensor((core, factors))\n",
    "\n",
    "    tk_RMSE = math.sqrt(np.square(np.subtract(X,tucker_reconstruction_2)).mean() )\n",
    "    print (\"Tucker \" + str(rank) + \" RMSE = \", tk_RMSE)\n",
    "    return core ,factors, tk_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d0d75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing Tucker decomposition\n",
    "from tensorly.decomposition import tucker\n",
    "import tensorly as tl\n",
    "\n",
    "ranks = []\n",
    "tucker_RMSE=[]\n",
    "\n",
    "for i in range(np.prod(X.shape)): # iterate through the flat index\n",
    "    rank= np.unravel_index(i, X.shape) # get the multidimensional index to use as a rank, in case it does not contain a zero, this will traverse all possible ranks\n",
    "    if np.all(rank):\n",
    "        core ,factors, tk_RMSE = doTucker(X, rank)\n",
    "        tucker_RMSE.append(tk_RMSE)\n",
    "        ranks.append(str(rank))\n",
    "\n",
    "\n",
    "max = np.argmin(tucker_RMSE)\n",
    "print (\"Lowest RMSE achieched at rank = \" + ranks[max]) # this will show the full matrix rank, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b7e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure( figsize=(20,6))\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "x_pos = [i for i, _ in enumerate(ranks)]\n",
    "\n",
    "plt.bar(ranks, tucker_RMSE, color='green')\n",
    "plt.xlabel(\"Dimensionality Reduction Methods\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"Comparing Classification RMSE using RandomForest on Linear unsupervised PCA, supervised LDA/QDA and various Embedding Learning Algorithms\")\n",
    "\n",
    "plt.xticks(x_pos, ranks)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e00df7d",
   "metadata": {},
   "source": [
    "## Looking at all possible ranks reconstruction error, it seems the highest errors occured when all modes were reduced, but when first mode only was reduced, the error was small, which means it is not very dominant in this dataset, and the third mode seems to be the most dominant.  \n",
    "\n",
    "In the book ( Kroonenberg, P.M., 2008. Applied multi-way data analysis, Wiley series in probability and statistics. Wiley-Interscience, Hoboken, N.J.) the author showed that rank (2,2,2) was the most fit to this dataset, which is proven here as well. The software he used can be downloaded from https://three-mode.leidenuniv.nl/ , "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
